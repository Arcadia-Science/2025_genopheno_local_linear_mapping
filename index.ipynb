{
 "cells": [
  {
   "cell_type": "raw",
   "id": "170215c2",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "---\n",
    "title: \"{{< var pub.title >}} --\"\n",
    "date: 'January 1, 2025'\n",
    "abstract-title: \"Summary\"\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42a2ab58-9efe-4758-a50d-8f9e011fe2cd",
   "metadata": {},
   "source": [
    "## Abstract\n",
    "\n",
    "Despite the header being `## Abstract`, this section will render as a highlighted section titled *Summary*. Ensure this section is a **maximum** of 280 characters.\n",
    "\n",
    "----\n",
    "\n",
    ":::{.callout-note title=\"AI usage disclosure\" collapse=\"true\"}\n",
    "This is a placeholder for the AI usage disclosure. Once all authors sign the AI code form on Airtable, SlackBot will message you an AI disclosure that you should place here.\n",
    ":::\n",
    "\n",
    "## Purpose\n",
    "\n",
    "This notebook fits a  2-layer MLP to a simple simulated genotype-phenotype dataset. This dataset contains 10,000 individuals with haploid genomes produced through random binomial sampling of 2 alleles. Hence there is no LD. Each individual expresses 5 phenotypes that range from purely additive (V_A/V_G = 1), to almost fully epistatic (V_A/V_G = 0.15). Epistatic interactions are strictly biallelic. Both additive and epistatic interactions are drawn from a gaussian distribution. Broad sense heritability is set to 0.99 so there is almost no environmental/measurement noise. \n",
    "\n",
    "Once the MLP model is fit on these data, we create a version with detached gradients in the ReLU activation functions, and calculate a Jacobian vector for each test-set sample to perform 'local linear approximation'. These calculations are done separately for each phenotype. Each element in the resulting phenotype-specific Jacobian vector represents a 'Locus Sensitivity' - the partial derivative of the predicted phenotype with respect to a specific genetic locus - which we can then explore to understand feature importance and interactions. \n",
    "\n",
    "## Learn more\n",
    "\n",
    "Refer to [Demo](/examples/demo.html) to learn about possible syntax."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d0510f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from sklearn.metrics import r2_score\n",
    "from scipy.stats import pearsonr\n",
    "import h5py\n",
    "from typing import cast, List, Tuple\n",
    "from pathlib import Path\n",
    "from collections import defaultdict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2b74082",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "hidden_size = 1024\n",
    "learning_rate = 0.001\n",
    "EPS = 1e-15\n",
    "\n",
    "sample_size = 10000\n",
    "qtl_n = 64\n",
    "rep = 1\n",
    "n_alleles = 2\n",
    "heritability = 1\n",
    "regularization = 1\n",
    "\n",
    "\n",
    "sim_name = f'qhaplo_{qtl_n}qtl_{sample_size}n_rep{rep}'\n",
    "base_file_name = f'sims/{sim_name}_'\n",
    "true_eff = pd.read_csv(f'sims/qhaplo_{qtl_n}qtl_{sample_size}n_rep1_eff.txt', delimiter=' ')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25ca18c5",
   "metadata": {},
   "source": [
    "#### Managing input data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27e29dad",
   "metadata": {},
   "outputs": [],
   "source": [
    "#datasets and dataloaders\n",
    "class BaseDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Base dataset class for loading data from HDF5 files.\n",
    "\n",
    "    Args:\n",
    "        hdf5_path: Path to the HDF5 file containing the data\n",
    "    \"\"\"\n",
    "    def __init__(self, hdf5_path: Path) -> None:\n",
    "        self.hdf5_path = hdf5_path\n",
    "        self.h5 = None\n",
    "        self._strain_group = None\n",
    "        self.strains = None\n",
    "        # Open temporarily to get keys and length for initialization\n",
    "        with h5py.File(self.hdf5_path, \"r\") as temp_h5:\n",
    "            temp_strain_group = cast(h5py.Group, temp_h5[\"strains\"])\n",
    "            self._strain_keys: List[str] = list(temp_strain_group.keys())\n",
    "            self._len = len(temp_strain_group)\n",
    "\n",
    "    def _init_h5(self):\n",
    "        if self.h5 is None:\n",
    "            self.h5 = h5py.File(self.hdf5_path, \"r\")\n",
    "            self._strain_group = cast(h5py.Group, self.h5[\"strains\"])\n",
    "            self.strains = self._strain_keys\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return self._len\n",
    "\n",
    "class GenoPhenoDataset(BaseDataset):\n",
    "    \"\"\"\n",
    "    Dataset for loading both genotype and phenotype data.\n",
    "\n",
    "    Returns tuples of (phenotype, genotype) tensors.\n",
    "\n",
    "    Args:\n",
    "        hdf5_path: Path to the HDF5 file containing the data\n",
    "        encode_minus_plus_one: If True, recode genotypes from 0/1 to -1/1\n",
    "    \"\"\"\n",
    "    def __init__(self, hdf5_path: Path, encode_minus_plus_one: bool = True) -> None:\n",
    "        super().__init__(hdf5_path)\n",
    "        self.encode_minus_plus_one = encode_minus_plus_one\n",
    "\n",
    "    def __getitem__(self, idx: int) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        self._init_h5()\n",
    "        strain = self.strains[idx]\n",
    "        strain_data = cast(Dataset, self._strain_group[strain])\n",
    "\n",
    "        phens = torch.tensor(strain_data[\"phenotype\"][:], dtype=torch.float32)\n",
    "        gens = torch.tensor(strain_data[\"genotype\"][:], dtype=torch.float32).flatten()\n",
    "\n",
    "        # Recode genotypes from 0/1 to -1/1 if requested\n",
    "        if self.encode_minus_plus_one:\n",
    "            gens = 2 * gens - 1  # This transforms 0→-1 and 1→1\n",
    "\n",
    "        return phens, gens\n",
    "\n",
    "class HeritabilityAdjustedDataset(torch.utils.data.Dataset):\n",
    "    \"\"\"\n",
    "    Dataset wrapper that adjusts phenotypes to achieve desired heritability\n",
    "    while maintaining a total variance of 1.\n",
    "\n",
    "    Args:\n",
    "        base_dataset: Original dataset providing (phenotype, genotype) tuples\n",
    "        target_heritability: Desired broad-sense heritability (0-1)\n",
    "        seed: Random seed for reproducibility\n",
    "    \"\"\"\n",
    "    def __init__(self, base_dataset, target_heritability=1.0, seed=42):\n",
    "        self.base_dataset = base_dataset\n",
    "        self.target_heritability = target_heritability\n",
    "\n",
    "        if target_heritability < 1.0:\n",
    "            # Calculate genetic scaling factor to achieve target heritability\n",
    "            # while maintaining total variance = 1\n",
    "            self.genetic_scale = np.sqrt(target_heritability)\n",
    "\n",
    "            # Calculate noise variance to make total variance = 1\n",
    "            self.noise_variance = 1 - target_heritability\n",
    "\n",
    "            # Pre-generate all noise patterns\n",
    "            rng = np.random.RandomState(seed)\n",
    "            self.noise_patterns = []\n",
    "\n",
    "            # Get shape of phenotypes by looking at first item\n",
    "            sample_phens, _ = base_dataset[0]\n",
    "            phen_shape = sample_phens.shape\n",
    "\n",
    "            # Generate noise for each sample\n",
    "            for i in range(len(base_dataset)):\n",
    "                noise = torch.tensor(\n",
    "                    rng.normal(0, np.sqrt(self.noise_variance), size=phen_shape),\n",
    "                    dtype=torch.float32\n",
    "                )\n",
    "                self.noise_patterns.append(noise)\n",
    "        else:\n",
    "            # No adjustment needed if heritability = 1\n",
    "            self.genetic_scale = 1.0\n",
    "            self.noise_patterns = None\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        phens, gens = self.base_dataset[idx]\n",
    "\n",
    "        if self.target_heritability < 1.0:\n",
    "            # Scale down genetic component\n",
    "            scaled_phens = phens * self.genetic_scale\n",
    "\n",
    "            # Add noise component\n",
    "            adjusted_phens = scaled_phens + self.noise_patterns[idx]\n",
    "        else:\n",
    "            adjusted_phens = phens\n",
    "\n",
    "        return adjusted_phens, gens\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.base_dataset)\n",
    "\n",
    "\n",
    "def create_data_loaders_with_heritability(base_file_name, heritability=1.0, batch_size=128,\n",
    "                                          num_workers=3, shuffle=True, seed=42,\n",
    "                                          encode_minus_plus_one=True):\n",
    "    \"\"\"\n",
    "    Create DataLoaders with adjusted heritability.\n",
    "\n",
    "    Args:\n",
    "        base_file_name: Base path for HDF5 files\n",
    "        heritability: Target broad-sense heritability (0-1)\n",
    "        batch_size: Batch size for DataLoaders\n",
    "        num_workers: Number of worker processes for DataLoaders\n",
    "        shuffle: Whether to shuffle the data\n",
    "        seed: Random seed for reproducibility\n",
    "        encode_minus_plus_one: If True, encode genotypes as -1/1 instead of 0/1\n",
    "\n",
    "    Returns:\n",
    "        Dictionary containing DataLoaders with adjusted heritability\n",
    "    \"\"\"\n",
    "    # Create base datasets\n",
    "    train_data_gp = GenoPhenoDataset(\n",
    "        Path(f'{base_file_name}train.hdf5'),\n",
    "        encode_minus_plus_one=encode_minus_plus_one\n",
    "    )\n",
    "    test_data_gp = GenoPhenoDataset(\n",
    "        Path(f'{base_file_name}test.hdf5'),\n",
    "        encode_minus_plus_one=encode_minus_plus_one\n",
    "    )\n",
    "\n",
    "    # Wrap with heritability adjustment\n",
    "    train_data_gp_adjusted = HeritabilityAdjustedDataset(train_data_gp, heritability, seed)\n",
    "    test_data_gp_adjusted = HeritabilityAdjustedDataset(test_data_gp, heritability, seed)\n",
    "\n",
    "    # Create DataLoaders\n",
    "    train_loader_gp = torch.utils.data.DataLoader(\n",
    "        dataset=train_data_gp_adjusted, batch_size=batch_size,\n",
    "        num_workers=num_workers, shuffle=shuffle\n",
    "    )\n",
    "    test_loader_gp = torch.utils.data.DataLoader(\n",
    "        dataset=test_data_gp_adjusted, batch_size=batch_size,\n",
    "        num_workers=num_workers, shuffle=shuffle\n",
    "    )\n",
    "\n",
    "    return {\n",
    "        'train_loader_gp': train_loader_gp,\n",
    "        'test_loader_gp': test_loader_gp,\n",
    "        'test_data_gp': test_data_gp_adjusted\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3926f621",
   "metadata": {},
   "outputs": [],
   "source": [
    "loaders = create_data_loaders_with_heritability(base_file_name, heritability=heritability)  # choose heritability\n",
    "train_loader_gp = loaders['train_loader_gp']\n",
    "test_loader_gp = loaders['test_loader_gp']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f1397d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Automatically extract n_phen and n_loci from the H5 file\n",
    "with h5py.File(f'{base_file_name}train.hdf5', 'r') as f:\n",
    "    # Get number of phenotypes\n",
    "    n_phen = len(f['metadata']['phenotype_names'])\n",
    "    print(f\"Number of phenotypes: {n_phen}\")\n",
    "\n",
    "    # Get number of loci\n",
    "    n_loci = len(f['metadata']['loci'])\n",
    "    print(f\"Number of loci: {n_loci}\")\n",
    "\n",
    "    # Optional: Print phenotype names\n",
    "    phenotype_names = [name.decode('utf-8') for name in f['metadata']['phenotype_names']]\n",
    "    print(f\"Phenotype names: {phenotype_names}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "244644ec",
   "metadata": {},
   "source": [
    "### Train 2-layer MLP "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bbec732",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GP_net(nn.Module):\n",
    "    \"\"\"\n",
    "    Modified fully connected G -> P network with Leaky ReLU for Jacobian analysis\n",
    "\n",
    "    Args:\n",
    "        n_loci: #QTLs\n",
    "        hidden_layer_size: geno hidden layer size\n",
    "        hidden_layer1_size: optional different size for 1st hidden layer\n",
    "        n_pheno: number of phenotypes to output/predict\n",
    "        detach_gate: whether to detach gradients in activation functions\n",
    "        leak: slope of the negative part of Leaky ReLU (default: 0.01)\n",
    "    \"\"\"\n",
    "    def __init__(self, n_loci, hidden_layer_size, n_pheno, hidden_layer1_size=None, detach_gate=False, leak=0.01):\n",
    "        super().__init__()\n",
    "\n",
    "        if hidden_layer1_size is None:\n",
    "            hidden_layer1_size = hidden_layer_size\n",
    "\n",
    "        # Create individual layers without bias\n",
    "        self.layer1 = nn.Linear(in_features=n_loci, out_features=hidden_layer1_size, bias=False)\n",
    "        self.layer2 = nn.Linear(in_features=hidden_layer1_size, out_features=hidden_layer_size, bias=False)\n",
    "        self.layer3 = nn.Linear(in_features=hidden_layer_size, out_features=n_pheno, bias=False)\n",
    "\n",
    "        # Store whether to detach gradients and leak parameter\n",
    "        self.detach_gate = detach_gate\n",
    "        self.leak = leak\n",
    "\n",
    "    def leaky_relu(self, x, detach=False):\n",
    "        \"\"\"\n",
    "        Custom Leaky ReLU implementation with optional gradient detachment\n",
    "        \"\"\"\n",
    "        if detach:\n",
    "            # Get positive mask with detached gradients\n",
    "            pos_mask = (x.clone().detach() > 0).float()\n",
    "\n",
    "            # Apply Leaky ReLU with detached gradients\n",
    "            return x * pos_mask + self.leak * x * (1 - pos_mask)\n",
    "        else:\n",
    "            # Regular Leaky ReLU\n",
    "            return torch.where(x > 0, x, self.leak * x)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # First layer with Leaky ReLU activation\n",
    "        x = self.layer1(x)\n",
    "        x = self.leaky_relu(x, detach=self.detach_gate)\n",
    "\n",
    "        # Second layer with Leaky ReLU activation\n",
    "        x = self.layer2(x)\n",
    "        x = self.leaky_relu(x, detach=self.detach_gate)\n",
    "\n",
    "        # Output layer (no activation)\n",
    "        x = self.layer3(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4d1e296",
   "metadata": {},
   "outputs": [],
   "source": [
    "#training loop\n",
    "def train_gpnet(model, train_loader, test_loader=None,\n",
    "                         n_loci=n_loci,\n",
    "                         n_alleles=2,\n",
    "                         max_epochs=100,  # Set a generous upper limit\n",
    "                         patience=10,      # Number of epochs to wait for improvement\n",
    "                         min_delta=0.003, # Minimum change to count as improvement\n",
    "                         learning_rate=None, weight_decay=regularization, device=device):\n",
    "    \"\"\"\n",
    "    Train model with early stopping to prevent overtraining\n",
    "    \"\"\"\n",
    "    # Move model to device\n",
    "    model = model.to(device)\n",
    "\n",
    "    # Initialize optimizer with proper weight decay\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "\n",
    "    # Learning rate scheduler\n",
    "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
    "        optimizer, mode='min', factor=0.5, patience=3\n",
    "    )\n",
    "\n",
    "    history = {\n",
    "        'train_loss': [],\n",
    "        'test_loss': [],\n",
    "        'epochs_trained': 0\n",
    "    }\n",
    "\n",
    "    # Early stopping variables\n",
    "    best_loss = float('inf')\n",
    "    best_epoch = 0\n",
    "    best_model_state = None\n",
    "    patience_counter = 0\n",
    "\n",
    "    # Training loop\n",
    "    for epoch in range(max_epochs):\n",
    "        # Training\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "\n",
    "        for i, (phens, gens) in enumerate(train_loader):\n",
    "\n",
    "            phens = phens.to(device)\n",
    "            gens = gens[:, : n_loci * n_alleles]\n",
    "            gens = gens.to(device)\n",
    "            gens_simplified = gens[:, 1::2]\n",
    "\n",
    "            # Forward pass\n",
    "            optimizer.zero_grad()\n",
    "            output = model(gens_simplified)\n",
    "\n",
    "            # focal loss\n",
    "            g_p_recon_loss = F.l1_loss(output + EPS, phens + EPS)\n",
    "\n",
    "            # Backward and optimize\n",
    "            g_p_recon_loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            train_loss += g_p_recon_loss.item()\n",
    "\n",
    "        avg_train_loss = train_loss / len(train_loader)\n",
    "\n",
    "        # Validation\n",
    "        if test_loader is not None:\n",
    "            model.eval()\n",
    "            test_loss = 0\n",
    "\n",
    "            with torch.no_grad():\n",
    "                for phens, gens in test_loader:\n",
    "                    phens = phens.to(device)\n",
    "                    gens = gens[:, : n_loci * n_alleles]\n",
    "                    gens = gens.to(device)\n",
    "                    gens_simplified = gens[:, 1::2]\n",
    "\n",
    "                    output = model(gens_simplified)\n",
    "                    test_loss += F.l1_loss(output + EPS, phens + EPS)\n",
    "\n",
    "            avg_test_loss = test_loss / len(test_loader)\n",
    "\n",
    "            if (epoch + 1) % 10 == 0:\n",
    "                print(f'Epoch: {epoch+1}/{max_epochs}, Train Loss: {avg_train_loss:.6f}, '\n",
    "                    f'Test Loss: {avg_test_loss:.6f}')\n",
    "\n",
    "            # Update learning rate\n",
    "            scheduler.step(avg_test_loss)\n",
    "\n",
    "            # Check for improvement\n",
    "            if avg_test_loss < (best_loss - min_delta):\n",
    "                best_loss = avg_test_loss\n",
    "                best_epoch = epoch\n",
    "                patience_counter = 0\n",
    "                # Save best model state\n",
    "                best_model_state = {k: v.cpu().detach().clone() for k, v in model.state_dict().items()}\n",
    "            else:\n",
    "                patience_counter += 1\n",
    "\n",
    "            # Early stopping check\n",
    "            if patience_counter >= patience:\n",
    "                print(f\"Early stopping triggered after {epoch+1} epochs\")\n",
    "                break\n",
    "\n",
    "    # Record how many epochs were actually used\n",
    "    history['epochs_trained'] = epoch + 1\n",
    "\n",
    "    # Restore best model\n",
    "    if best_model_state is not None:\n",
    "        print(f\"Restoring best model from epoch {best_epoch+1}\")\n",
    "        model.load_state_dict(best_model_state)\n",
    "\n",
    "    return model, best_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7987d7f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Run MLP training\n",
    "model = GP_net(\n",
    "    n_loci=n_loci,\n",
    "    hidden_layer_size=hidden_size,\n",
    "    n_pheno=n_phen\n",
    "    )\n",
    "\n",
    "# Use early stopping with appropriate patience\n",
    "model, best_loss_gp = train_gpnet(model=model,\n",
    "                                    train_loader=train_loader_gp,\n",
    "                                    test_loader=test_loader_gp,\n",
    "                                    n_loci=n_loci,\n",
    "                                    learning_rate=learning_rate,\n",
    "                                    device=device)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba06d17d",
   "metadata": {},
   "source": [
    "### Verify model performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3a3b6c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#model performance results\n",
    "true_phenotypes = []\n",
    "predicted_phenotypes = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for phens, gens in test_loader_gp:\n",
    "        phens = phens.to(device)\n",
    "        gens = gens.to(device)\n",
    "        gens_simplified = gens[:, 1::2]\n",
    "\n",
    "        # Get predictions\n",
    "        predictions = model(gens_simplified)\n",
    "\n",
    "        # Store results\n",
    "        true_phenotypes.append(phens.cpu().numpy())\n",
    "        predicted_phenotypes.append(predictions.cpu().numpy())\n",
    "\n",
    "    # Concatenate batches\n",
    "    true_phenotypes = np.concatenate(true_phenotypes)\n",
    "    predicted_phenotypes = np.concatenate(predicted_phenotypes)\n",
    "\n",
    "    # Calculate correlations for each phenotype\n",
    "    correlations = []\n",
    "    r2s = []\n",
    "\n",
    "    for i in range(n_phen):\n",
    "        corr, _ = pearsonr(true_phenotypes[:, i], predicted_phenotypes[:, i])\n",
    "        correlations.append(corr)\n",
    "\n",
    "        r2 = r2_score(true_phenotypes[:, i], predicted_phenotypes[:, i])\n",
    "        r2s.append(r2)\n",
    "\n",
    "    true_h2 = [1, 0.787, 0.505, 0.403, 0.122]\n",
    "\n",
    "    # Create a detailed DataFrame with all results\n",
    "    results_df_fit = pd.DataFrame({\n",
    "        'trait': phenotype_names,\n",
    "        'trait_index': range(1, n_phen + 1),\n",
    "        'pearson_correlation': correlations,\n",
    "        'r2': r2s,\n",
    "        'relAA': true_h2\n",
    "    })"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7a51304",
   "metadata": {},
   "source": [
    "Test set prediction statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab9019c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df_fit['relAA'] = results_df_fit['relAA'] * heritability\n",
    "\n",
    "results_df_fit['VAA_base'] =      (1 - results_df_fit['relAA'])\n",
    "results_df_fit['VA_base'] =     (results_df_fit['relAA'])\n",
    "results_df_fit['H2_captured'] = results_df_fit['r2']/heritability\n",
    "\n",
    "results_df_fit['VAA_captured'] =    (results_df_fit['r2'] - results_df_fit['VA_base'] )/ results_df_fit['VAA_base']\n",
    "\n",
    "\n",
    "results_df_fit.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65ed9f94",
   "metadata": {},
   "source": [
    "## local linear mapping"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffaa9bd3",
   "metadata": {},
   "source": [
    "### Test detached jacobian reconstruction accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b081227c",
   "metadata": {},
   "outputs": [],
   "source": [
    "detached_model = GP_net(\n",
    "    n_loci=n_loci,\n",
    "    hidden_layer_size=hidden_size,\n",
    "    n_pheno=n_phen,\n",
    "    detach_gate=True\n",
    "    )\n",
    "\n",
    "def plot_jacobian_vs_predictions(model, test_loader, n_phenotypes=5, max_batches=20, device=device):\n",
    "    dot_products = [[] for _ in range(n_phenotypes)]\n",
    "    predictions = [[] for _ in range(n_phenotypes)]\n",
    "\n",
    "    def get_jacobian(x, idx):\n",
    "        def fn(inp): return model(inp)[:, idx]\n",
    "        return torch.autograd.functional.jacobian(fn, x, vectorize=True).squeeze()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (phens, gens) in enumerate(test_loader):\n",
    "            if batch_idx >= max_batches: break\n",
    "\n",
    "            gens = gens.to(device)\n",
    "            gens_simplified = gens[:, 1::2] if gens.shape[1] > n_loci else gens\n",
    "\n",
    "            for i in range(gens.shape[0]):\n",
    "                single_gens = gens_simplified[i:i+1]\n",
    "                model_preds = model(single_gens)[0]\n",
    "\n",
    "                for p_idx in range(n_phenotypes):\n",
    "                    predictions[p_idx].append(model_preds[p_idx].item())\n",
    "                    jacobian = get_jacobian(single_gens, p_idx)\n",
    "                    dot_products[p_idx].append(torch.sum(jacobian * single_gens).item())\n",
    "\n",
    "    fig, axes = plt.subplots(1, n_phenotypes, figsize=(n_phenotypes * 4, 4))\n",
    "\n",
    "    for p_idx in range(n_phenotypes):\n",
    "        dots = np.array(dot_products[p_idx])\n",
    "        preds = np.array(predictions[p_idx])\n",
    "        r2 = np.corrcoef(dots, preds)[0, 1]**2\n",
    "\n",
    "        axes[p_idx].scatter(dots, preds, alpha=0.5)\n",
    "        min_val, max_val = min(min(dots), min(preds)), max(max(dots), max(preds))\n",
    "        axes[p_idx].plot([min_val, max_val], [min_val, max_val], 'r--')\n",
    "        axes[p_idx].set_title(f'Phenotype {p_idx+1}, R² = {r2:.2f}')\n",
    "        axes[p_idx].set_xlabel('Jacobian*input')\n",
    "        axes[p_idx].set_ylabel('MLP pred')\n",
    "\n",
    "    fig.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    return dot_products, predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f794c4e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "dot_products, predictions = plot_jacobian_vs_predictions(detached_model, test_loader_gp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f86933a",
   "metadata": {},
   "source": [
    "### linear locus sensitivity based on detached jacobian\n",
    "\n",
    "First generate sensitivity value for each locus, for each phenotype, for each test set sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c066c0f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_feature_importance_across_validation(model, validation_loader, true_effects_df,\n",
    "                                                phenotype_idx=[0], device=device, max_samples=None):\n",
    "    \"\"\"\n",
    "    Calculate feature importance for each sample in the validation set and visualize distributions.\n",
    "\n",
    "    Args:\n",
    "        model: Trained neural network model\n",
    "        validation_loader: DataLoader for validation data\n",
    "        true_effects_df: DataFrame containing ground truth effects\n",
    "        phenotype_idx: List of indexes of phenotypes to analyze (default: [0] for first phenotype)\n",
    "        device: Device to run calculations on\n",
    "        max_samples: Maximum number of samples to process (None for all)\n",
    "\n",
    "    Returns:\n",
    "        List of DataFrames with importance values for each locus across all samples, one per phenotype\n",
    "    \"\"\"\n",
    "    # Convert single index to list for consistency\n",
    "    if isinstance(phenotype_idx, int):\n",
    "        phenotype_idx = [phenotype_idx]\n",
    "\n",
    "    # Create detached version of model for Jacobian calculation\n",
    "    detached_model = GP_net(\n",
    "    n_loci=n_loci,\n",
    "    hidden_layer_size=hidden_size,\n",
    "    n_pheno=n_phen,\n",
    "    detach_gate=True\n",
    "    )\n",
    "\n",
    "    # Copy weights from trained model\n",
    "    detached_model.load_state_dict(model.state_dict())\n",
    "    detached_model = detached_model.to(device)\n",
    "\n",
    "    # Function to get importance for a specific phenotype\n",
    "    def get_phenotype_importance(x, pheno_idx):\n",
    "        def phenotype_fn(inp):\n",
    "            return detached_model(inp)[:, pheno_idx]\n",
    "\n",
    "        # Calculate Jacobian\n",
    "        jacobian = torch.autograd.functional.jacobian(\n",
    "            phenotype_fn,\n",
    "            x,\n",
    "            vectorize=True\n",
    "        ).squeeze()\n",
    "\n",
    "        # Return unmodified Jacobian value\n",
    "        return jacobian\n",
    "\n",
    "    # Store importance values for each sample and each phenotype\n",
    "    all_importance_values = {idx: [] for idx in phenotype_idx}\n",
    "    sample_count = 0\n",
    "\n",
    "    # Process validation samples\n",
    "    with torch.no_grad():  # No need for gradients in the forward pass\n",
    "        for phens, gens in validation_loader:\n",
    "            # Process each sample in the batch\n",
    "            for i in range(gens.shape[0]):\n",
    "                # Check if we've reached the maximum samples\n",
    "                if max_samples is not None and sample_count >= max_samples:\n",
    "                    break\n",
    "\n",
    "                # Get single sample\n",
    "                single_gens = gens[i:i+1]\n",
    "\n",
    "                # Flatten and simplify (remove one-hot encoding)\n",
    "                flattened_gens = single_gens.reshape(single_gens.shape[0], -1)\n",
    "                simplified_gens = flattened_gens[:, 1::2].to(device)\n",
    "\n",
    "                # Calculate importance for each phenotype\n",
    "                for idx in phenotype_idx:\n",
    "                    importance = get_phenotype_importance(simplified_gens, idx)\n",
    "                    # Store the importance values\n",
    "                    all_importance_values[idx].append(importance.cpu().numpy())\n",
    "\n",
    "                sample_count += 1\n",
    "\n",
    "                # Print progress\n",
    "                if sample_count % 1000 == 0:\n",
    "                    print(f\"Processed {sample_count} samples\")\n",
    "\n",
    "            # Check again after batch\n",
    "            if max_samples is not None and sample_count >= max_samples:\n",
    "                break\n",
    "\n",
    "    print(f\"Completed importance analysis for {sample_count} samples\")\n",
    "\n",
    "    # Convert to DataFrames for easier analysis\n",
    "    importance_dfs = {}\n",
    "    for idx in phenotype_idx:\n",
    "        importance_df = pd.DataFrame(all_importance_values[idx])\n",
    "        importance_df.columns = [f'Locus_{i}' for i in range(importance_df.shape[1])]\n",
    "        importance_dfs[idx] = importance_df\n",
    "\n",
    "    # Create boxplot visualization\n",
    "    plt.figure(figsize=(len(phenotype_idx) * 15, 7))\n",
    "\n",
    "    # Create subplots for scatter plots\n",
    "    fig2, axes2 = plt.subplots(1, len(phenotype_idx), figsize=(len(phenotype_idx) * 5, 5))\n",
    "    if len(phenotype_idx) == 1:\n",
    "        axes2 = [axes2]  # Convert to list for consistent indexing\n",
    "\n",
    "    for i, idx in enumerate(phenotype_idx):\n",
    "        # Get the dataframe for this phenotype\n",
    "        importance_df = importance_dfs[idx]\n",
    "\n",
    "        # Extract ground truth for the specific phenotype\n",
    "        true_effects = true_effects_df[true_effects_df['trait'] == idx + 1]\n",
    "\n",
    "        # Generate correlation plot between median importance and true effects\n",
    "        if 'add_eff' in true_effects.columns:\n",
    "            # Get data for scatter plot\n",
    "            true_effects_values = true_effects['add_eff'] * np.sqrt(heritability)\n",
    "            mean_importance = importance_df.mean().values\n",
    "\n",
    "            # Create scatter plot\n",
    "            axes2[i].scatter(true_effects_values, mean_importance, alpha=0.7)\n",
    "\n",
    "            # Add best fit line\n",
    "            z = np.polyfit(true_effects_values, mean_importance, 1)\n",
    "            p = np.poly1d(z)\n",
    "            axes2[i].plot(true_effects_values, p(true_effects_values), \"r--\", alpha=0.7)\n",
    "\n",
    "            # Add correlation coefficient\n",
    "            corr = np.corrcoef(true_effects_values, mean_importance)[0, 1]\n",
    "            axes2[i].set_xlabel('Add effect size', fontsize=12)\n",
    "            axes2[i].set_ylabel('Mean Locus Sensitivity', fontsize=12)\n",
    "            axes2[i].set_title(f'Phenotype {idx+1}: r = {corr:.3f}')\n",
    "            # Add this after the correlation coefficient calculation\n",
    "            slope = z[0]  # Extract the slope from the polyfit result\n",
    "            axes2[i].text(0.05, 0.95, f'Slope = {slope:.3f}',\n",
    "                        transform=axes2[i].transAxes,\n",
    "                        verticalalignment='top',\n",
    "                        bbox=dict(boxstyle='round', facecolor='white', alpha=0.7))\n",
    "\n",
    "    fig2.tight_layout()\n",
    "\n",
    "    # Show both sets of plots\n",
    "    plt.show()\n",
    "\n",
    "    # Return the DataFrames for further analysis\n",
    "    return list(importance_dfs.values())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8290e331",
   "metadata": {},
   "source": [
    "Visualize variance in locus sensitity for each locus between individuals\n",
    "\n",
    "\n",
    "Visualize concordance between mean sensitivity and true additive effect size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66a05cb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Usage example:\n",
    "importance_results = analyze_feature_importance_across_validation(\n",
    "    model=model,\n",
    "    validation_loader=test_loader_gp,\n",
    "    true_effects_df=true_eff,\n",
    "    phenotype_idx=[0,1,2,3,4],  #  phenotype\n",
    "    device=device,\n",
    "    max_samples=2000  # Limit number of samples for faster processing\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4703241",
   "metadata": {},
   "source": [
    "Notice that the variance in sensitivity is directly a product of epistasis. \n",
    "\n",
    "Loci that are involved in epistatic interactions vary in their contribution to phenotype between samples as their effects are context dependent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7053b79",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_sensitivity_variance_vs_epistasis(importance_dfs, true_effects_df, phenotype_idx=[0]):\n",
    "    \"\"\"\n",
    "    Plot the variance in locus sensitivity across samples versus epistatic effects,\n",
    "    with a quadratic fit curve and equation.\n",
    "\n",
    "    Args:\n",
    "        importance_dfs: List of DataFrames with importance values, one per phenotype\n",
    "        true_effects_df: DataFrame containing ground truth effects\n",
    "        phenotype_idx: List of indexes of phenotypes to analyze (default: [0] for first phenotype)\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    # Convert single index to list for consistency\n",
    "    if isinstance(phenotype_idx, int):\n",
    "        phenotype_idx = [phenotype_idx]\n",
    "\n",
    "    # Create subplots\n",
    "    fig, axes = plt.subplots(1, len(phenotype_idx), figsize=(len(phenotype_idx) * 4, 4))\n",
    "    if len(phenotype_idx) == 1:\n",
    "        axes = [axes]  # Convert to list for consistent indexing\n",
    "\n",
    "    for i, idx in enumerate(phenotype_idx):\n",
    "        # Get the dataframe for this phenotype\n",
    "        if isinstance(importance_dfs, list):\n",
    "            importance_df = importance_dfs[i]\n",
    "        else:\n",
    "            importance_df = importance_dfs[idx]\n",
    "\n",
    "        # Calculate variance for each locus\n",
    "        locus_variance = importance_df.var().values\n",
    "\n",
    "        # Extract ground truth for the specific phenotype\n",
    "        true_effects = true_effects_df[true_effects_df['trait'] == idx + 1]\n",
    "\n",
    "        # Extract epistatic effects\n",
    "        epistatic_effects = true_effects['epi_eff'].values\n",
    "        epistatic_effects = epistatic_effects * np.sqrt(heritability)\n",
    "\n",
    "        # Create scatter plot\n",
    "        axes[i].scatter(epistatic_effects, locus_variance, alpha=0.7, color='blue',\n",
    "                       label='Data points')\n",
    "\n",
    "        # Fit quadratic curve (y = ax² + bx + c)\n",
    "        if len(epistatic_effects) > 2:  # Need at least 3 points for quadratic fit\n",
    "            from numpy.polynomial.polynomial import polyfit\n",
    "\n",
    "            # Polynomial fit (degree 2 for quadratic)\n",
    "            c, b, a = polyfit(epistatic_effects, locus_variance, 2)\n",
    "\n",
    "            # Generate points for the fit curve\n",
    "            x_fit = np.linspace(min(epistatic_effects), max(epistatic_effects), 100)\n",
    "            y_fit = a * x_fit**2 + b * x_fit + c\n",
    "\n",
    "            # Plot the fit curve\n",
    "            axes[i].plot(x_fit, y_fit, 'r-', linewidth=2, label='Quadratic fit')\n",
    "\n",
    "            # Add the equation to the plot\n",
    "            equation = f'y = {a:.4f}x² + ...'\n",
    "\n",
    "            # Calculate R-squared\n",
    "            y_pred = a * np.array(epistatic_effects)**2 + b * np.array(epistatic_effects) + c\n",
    "            ss_total = np.sum((locus_variance - np.mean(locus_variance))**2)\n",
    "            ss_residual = np.sum((locus_variance - y_pred)**2)\n",
    "            r_squared = 1 - (ss_residual / ss_total)\n",
    "\n",
    "            # Add equation and R² to the plot\n",
    "            axes[i].text(0.05, 0.95, f'{equation}\\nR² = {r_squared:.4f}',\n",
    "                        transform=axes[i].transAxes, fontsize=10,\n",
    "                        verticalalignment='top', bbox=dict(boxstyle='round', facecolor='white', alpha=0.7))\n",
    "\n",
    "        # Add labels and title\n",
    "        axes[i].set_xlabel('Epistatic Effect Size', fontsize=12)\n",
    "        axes[i].set_ylabel('Locus Sensitivity Variance', fontsize=12)\n",
    "        axes[i].set_title(f'Phenotype {idx+1}', fontsize=14)\n",
    "\n",
    "        # Add legend\n",
    "        if len(epistatic_effects) > 2:\n",
    "            axes[i].legend()\n",
    "\n",
    "    # Standardize y-axis limits across all subplots\n",
    "    y_min = min([ax.get_ylim()[0] for ax in axes])\n",
    "    y_max = max([ax.get_ylim()[1] for ax in axes])\n",
    "    for ax in axes:\n",
    "        ax.set_ylim([y_min, y_max])\n",
    "\n",
    "    # Adjust layout\n",
    "    fig.tight_layout()\n",
    "    fig.suptitle('Relationship Between Epistatic Effect Size and Variance in Locus Sensitivity',\n",
    "                fontsize=16, y=1.05)\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f60263c3",
   "metadata": {},
   "source": [
    "We can demonstrate this by plotting locus sensitivity variance (among individuals) vs. the epistatic effect size of the locus. A quadratic relationship emerges due to the y axis plotting a variance term. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e91c309",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_sensitivity_variance_vs_epistasis(importance_results, true_eff, phenotype_idx=[0, 1, 2, 3 ,4])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2139e7c",
   "metadata": {},
   "source": [
    "We can also hunt for epistatic couplings by calculating the covariance in sensitivity values across pairs of loci. \n",
    "\n",
    "Pairwise interactors should show some signal of covariation in sensitivity (as they affect each other when found together). \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7ae21e1",
   "metadata": {},
   "source": [
    "Finally we can test how much the Jacobian vectors vary from sample to sample to check just how 'local' the reconstruction is. As expected, for additive phenotypes, the reconstruction turns out to be quite global, with most samples having near identical Jacobian vectors. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d00e4322",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_top_epistatic_pairs(model, validation_loader, true_effects_df,\n",
    "                               top_n=10, phenotype_idx=None, device='cuda',\n",
    "                               max_samples=1000, plot_summary=True,\n",
    "                               selection_type='top', genotype_encoding=(-1, 1)):\n",
    "    \"\"\"\n",
    "    Extract the top or bottom N strongest epistatic pairs for each phenotype, along with\n",
    "    their raw sensitivity values and combined genotypes. Only includes unique\n",
    "    locus combinations (eliminates duplicates like locus1/locus2 and locus2/locus1).\n",
    "\n",
    "    Returns a dictionary of detailed results and a summarized DataFrame with one row\n",
    "    per phenotype/genotype/locus pair, with separate columns for genotype1 and genotype2.\n",
    "\n",
    "    Args:\n",
    "        model: Trained neural network model\n",
    "        validation_loader: DataLoader for validation data\n",
    "        true_effects_df: DataFrame containing ground truth effects (must have epi_eff column)\n",
    "        top_n: Number of top/bottom epistatic pairs to extract per phenotype\n",
    "        phenotype_idx: List of phenotype indices to analyze (None for all phenotypes)\n",
    "        device: Device to run calculations on\n",
    "        max_samples: Maximum number of samples to process\n",
    "        plot_summary: Whether to plot summary visualizations\n",
    "        selection_type: 'top' to select largest magnitude epistatic effects, 'bottom' for smallest\n",
    "        genotype_encoding: Tuple of (low, high) values used for encoding genotypes (default: (-1, 1))\n",
    "\n",
    "    Returns:\n",
    "        tuple: (results_dict, summary_df)\n",
    "            - results_dict: Dictionary with detailed results\n",
    "            - summary_df: DataFrame with one row per phenotype/genotype/locus pair\n",
    "    \"\"\"\n",
    "    # Validate selection_type\n",
    "    if selection_type not in ['top', 'bottom']:\n",
    "        raise ValueError(\"selection_type must be either 'top' or 'bottom'\")\n",
    "\n",
    "    # Extract genotype encoding values\n",
    "    low_encoding, high_encoding = genotype_encoding\n",
    "\n",
    "    # Function to convert between -1/1 and 0/1 encoding (for display purposes)\n",
    "    def to_binary_encoding(genotype):\n",
    "        if genotype == low_encoding:\n",
    "            return 0\n",
    "        elif genotype == high_encoding:\n",
    "            return 1\n",
    "        else:\n",
    "            raise ValueError(f\"Unexpected genotype value: {genotype}\")\n",
    "\n",
    "    # Ensure true_effects_df has required columns\n",
    "    required_cols = ['trait', 'locus', 'epi_loc', 'epi_eff']\n",
    "    if not all(col in true_effects_df.columns for col in required_cols):\n",
    "        raise ValueError(f\"true_effects_df must have these columns: {required_cols}\")\n",
    "\n",
    "    # Check for additive effects columns\n",
    "    if 'add_eff' in true_effects_df.columns:\n",
    "        add_eff_col = 'add_eff'\n",
    "\n",
    "    # Determine phenotype indices to analyze\n",
    "    if phenotype_idx is None:\n",
    "        # Get all unique phenotypes from true_effects_df\n",
    "        all_traits = true_effects_df['trait'].unique()\n",
    "        # Convert to 0-based indexing if needed\n",
    "        phenotype_idx = [int(trait) - 1 if trait >= 1 else int(trait) for trait in all_traits]\n",
    "    elif isinstance(phenotype_idx, int):\n",
    "        phenotype_idx = [phenotype_idx]\n",
    "\n",
    "    print(f\"Analyzing phenotypes: {[idx+1 for idx in phenotype_idx]}\")\n",
    "    print(f\"Selection type: {selection_type} {top_n} epistatic pairs\")\n",
    "    print(f\"Genotype encoding: {genotype_encoding}\")\n",
    "\n",
    "    # Create detached version of model for Jacobian calculation (copied from your function)\n",
    "    detached_model = GP_net(\n",
    "        n_loci=n_loci,\n",
    "        hidden_layer_size=hidden_size,\n",
    "        n_pheno=n_phen,\n",
    "        detach_gate=True\n",
    "        )\n",
    "\n",
    "    detached_model.load_state_dict(model.state_dict())\n",
    "    detached_model = detached_model.to(device)\n",
    "\n",
    "\n",
    "    # Function to get importance for a specific phenotype (copied from your function)\n",
    "    def get_phenotype_importance(x, pheno_idx):\n",
    "        def phenotype_fn(inp):\n",
    "            return detached_model(inp)[:, pheno_idx]\n",
    "\n",
    "        # Calculate Jacobian\n",
    "        jacobian = torch.autograd.functional.jacobian(\n",
    "            phenotype_fn,\n",
    "            x,\n",
    "            vectorize=True\n",
    "        ).squeeze()\n",
    "\n",
    "        # Return unmodified Jacobian value\n",
    "        return jacobian\n",
    "\n",
    "    # Find top/bottom N epistatic pairs for each phenotype\n",
    "# Find top/bottom N epistatic pairs for each phenotype\n",
    "    top_pairs = {}\n",
    "    for idx in phenotype_idx:\n",
    "        # Filter true effects for this phenotype\n",
    "        trait_idx = idx + 1  # Convert 0-based to 1-based indexing\n",
    "        pheno_effects = true_effects_df[true_effects_df['trait'] == trait_idx]\n",
    "\n",
    "        # Get non-zero effects (avoid selecting pairs with no epistasis)\n",
    "        pheno_effects = pheno_effects[pheno_effects['epi_eff'] != 0]\n",
    "\n",
    "        # Calculate absolute epistatic effect\n",
    "        pheno_effects['abs_epi_eff'] = np.abs(pheno_effects['epi_eff'])\n",
    "\n",
    "        # Sort by absolute epistatic effect\n",
    "        if selection_type == 'top':\n",
    "            # Largest absolute effects first (descending)\n",
    "            sorted_effects = pheno_effects.sort_values('abs_epi_eff', ascending=False)\n",
    "        else:  # 'bottom'\n",
    "            # Smallest absolute effects first (ascending)\n",
    "            sorted_effects = pheno_effects.sort_values('abs_epi_eff', ascending=True)\n",
    "\n",
    "        # Create set to track unique locus pairs\n",
    "        unique_pairs = set()\n",
    "        pairs = []\n",
    "\n",
    "        # Process pairs in sorted order\n",
    "        for _, row in sorted_effects.iterrows():\n",
    "            # Convert to 0-based indexing if needed\n",
    "            locus1 = int(row['locus']) - 1 if row['locus'] >= 1 else int(row['locus'])\n",
    "            locus2 = int(row['epi_loc']) - 1 if row['epi_loc'] >= 1 else int(row['epi_loc'])\n",
    "\n",
    "            # Create a canonical representation of the pair (always smaller locus first)\n",
    "            # This ensures we don't count both (a,b) and (b,a) as separate pairs\n",
    "            pair_key = tuple(sorted([locus1, locus2]))\n",
    "\n",
    "            # Skip if we've already seen this pair\n",
    "            if pair_key in unique_pairs:\n",
    "                continue\n",
    "\n",
    "            # Add to set of unique pairs\n",
    "            unique_pairs.add(pair_key)\n",
    "\n",
    "            # Get additive effects\n",
    "            if add_eff_col is not None:\n",
    "                # Get additive effect for locus1\n",
    "                add_eff1 = row[add_eff_col]\n",
    "\n",
    "                # Get additive effect for locus2 (might need to find it in another row)\n",
    "                if 'add_eff2' in row and not pd.isna(row['add_eff2']):\n",
    "                    add_eff2 = row['add_eff2']\n",
    "                else:\n",
    "                    # Try to find locus2's additive effect in another row\n",
    "                    locus2_row = pheno_effects[pheno_effects['locus'] == row['epi_loc']]\n",
    "                    if len(locus2_row) > 0:\n",
    "                        add_eff2 = locus2_row.iloc[0][add_eff_col]\n",
    "                    else:\n",
    "                        add_eff2 = None\n",
    "            else:\n",
    "                add_eff1 = None\n",
    "                add_eff2 = None\n",
    "\n",
    "            # Store pair info\n",
    "            pairs.append({\n",
    "                'locus1': locus1,\n",
    "                'locus2': locus2,\n",
    "                'epi_eff': row['epi_eff'],\n",
    "                'abs_epi_eff': row['abs_epi_eff'],\n",
    "                'add_eff1': add_eff1,\n",
    "                'add_eff2': add_eff2\n",
    "            })\n",
    "\n",
    "            # Break once we have top_n unique pairs\n",
    "            if len(pairs) >= top_n:\n",
    "                break\n",
    "\n",
    "        top_pairs[idx] = pairs\n",
    "        print(f\"Selected {len(pairs)} unique {selection_type} epistatic pairs for phenotype {idx+1}\")\n",
    "\n",
    "        # Print magnitude range\n",
    "        if len(pairs) > 0:\n",
    "            min_mag = min(pair['abs_epi_eff'] for pair in pairs)\n",
    "            max_mag = max(pair['abs_epi_eff'] for pair in pairs)\n",
    "            print(f\"  Epistatic effect magnitude range: {min_mag:.6f} to {max_mag:.6f}\")\n",
    "\n",
    "\n",
    "    # Process validation samples to get sensitivities and genotypes\n",
    "    results = {}\n",
    "    sample_indices = []\n",
    "    genotypes_dict = {}\n",
    "    sensitivities_dict = defaultdict(lambda: defaultdict(list))\n",
    "\n",
    "    # Process validation samples\n",
    "    sample_count = 0\n",
    "    with torch.no_grad():  # No need for gradients in the forward pass\n",
    "        for batch_idx, (phens, gens) in enumerate(validation_loader):\n",
    "            # Process each sample in the batch\n",
    "            for i in range(gens.shape[0]):\n",
    "                # Check if we've reached the maximum samples\n",
    "                if max_samples is not None and sample_count >= max_samples:\n",
    "                    break\n",
    "\n",
    "                # Store sample index\n",
    "                sample_indices.append(sample_count)\n",
    "\n",
    "                # Get single sample\n",
    "                single_gens = gens[i:i+1]\n",
    "\n",
    "                # Flatten and simplify (remove one-hot encoding)\n",
    "                flattened_gens = single_gens.reshape(single_gens.shape[0], -1)\n",
    "                simplified_gens = flattened_gens[:, 1::2].to(device)\n",
    "\n",
    "                # Store the raw genotype data\n",
    "                genotype = simplified_gens.cpu().numpy().squeeze()\n",
    "                genotypes_dict[sample_count] = genotype\n",
    "\n",
    "                # Calculate importance for each phenotype\n",
    "                for idx in phenotype_idx:\n",
    "                    importance = get_phenotype_importance(simplified_gens, idx)\n",
    "                    importance_np = importance.cpu().numpy()\n",
    "\n",
    "                    # Store sensitivity values for each locus in top pairs\n",
    "                    for pair in top_pairs[idx]:\n",
    "                        locus1 = pair['locus1']\n",
    "                        locus2 = pair['locus2']\n",
    "                        pair_key = (idx, locus1, locus2)\n",
    "\n",
    "                        # Store sensitivity values\n",
    "                        sensitivities_dict[pair_key]['locus1'].append(importance_np[locus1])\n",
    "                        sensitivities_dict[pair_key]['locus2'].append(importance_np[locus2])\n",
    "\n",
    "                sample_count += 1\n",
    "\n",
    "                # Print progress\n",
    "                if sample_count % 1000 == 0:\n",
    "                    print(f\"Processed {sample_count} samples\")\n",
    "\n",
    "            # Check again after batch\n",
    "            if max_samples is not None and sample_count >= max_samples:\n",
    "                break\n",
    "\n",
    "    print(f\"Completed analysis for {sample_count} samples\")\n",
    "\n",
    "    # Prepare final results\n",
    "    results = {}\n",
    "    summary_rows = []  # For the summarized DataFrame\n",
    "\n",
    "    # Get phenotype names (trait names) if available\n",
    "    trait_names = {}\n",
    "    if 'trait_name' in true_effects_df.columns:\n",
    "        for _, row in true_effects_df.drop_duplicates('trait').iterrows():\n",
    "            trait_names[row['trait'] - 1] = row['trait_name']  # Convert to 0-based index\n",
    "\n",
    "    for idx in phenotype_idx:\n",
    "        # Get phenotype name if available, otherwise use index\n",
    "        phenotype_name = trait_names.get(idx, f\"Phenotype_{idx+1}\")\n",
    "\n",
    "        for pair in top_pairs[idx]:\n",
    "            locus1 = pair['locus1']\n",
    "            locus2 = pair['locus2']\n",
    "            pair_key = (idx, locus1, locus2)\n",
    "\n",
    "            # Get sensitivity values\n",
    "            locus1_sensitivities = np.array(sensitivities_dict[pair_key]['locus1'])\n",
    "            locus2_sensitivities = np.array(sensitivities_dict[pair_key]['locus2'])\n",
    "\n",
    "            # Get genotypes for this pair\n",
    "            genotype1_values = []\n",
    "            genotype2_values = []\n",
    "            combined_genotypes = []\n",
    "\n",
    "            for s_idx in sample_indices:\n",
    "                genotype = genotypes_dict[s_idx]\n",
    "\n",
    "                # Extract genotype values for the two loci\n",
    "                geno1 = genotype[locus1]\n",
    "                geno2 = genotype[locus2]\n",
    "\n",
    "                # Store individual genotypes\n",
    "                genotype1_values.append(geno1)\n",
    "                genotype2_values.append(geno2)\n",
    "\n",
    "                # Encode combined genotype - modified for -1/1 encoding\n",
    "                # For display purposes we convert to 0/1 encoding (00, 01, 10, 11)\n",
    "                # The combined value will be: 0 (low,low), 1 (low,high), 2 (high,low), 3 (high,high)\n",
    "                binary_geno1 = 1 if geno1 == high_encoding else 0\n",
    "                binary_geno2 = 1 if geno2 == high_encoding else 0\n",
    "                combined = binary_geno1 * 2 + binary_geno2\n",
    "                combined_genotypes.append(combined)\n",
    "\n",
    "            # Convert to numpy arrays\n",
    "            genotype1_values = np.array(genotype1_values)\n",
    "            genotype2_values = np.array(genotype2_values)\n",
    "            combined_genotypes = np.array(combined_genotypes)\n",
    "\n",
    "            # Store detailed results\n",
    "            results[pair_key] = {\n",
    "                'locus1_sensitivities': locus1_sensitivities,\n",
    "                'locus2_sensitivities': locus2_sensitivities,\n",
    "                'genotype1_values': genotype1_values,\n",
    "                'genotype2_values': genotype2_values,\n",
    "                'combined_genotypes': combined_genotypes,\n",
    "                'add_eff1': pair['add_eff1'],\n",
    "                'add_eff2': pair['add_eff2'],\n",
    "                'epi_eff': pair['epi_eff'],\n",
    "                'abs_epi_eff': pair['abs_epi_eff'],\n",
    "                'samples': np.array(sample_indices)\n",
    "            }\n",
    "\n",
    "            # Calculate mean sensitivity values for each genotype combination\n",
    "            # We use the actual encoding values, not 0/1\n",
    "            for geno1 in [low_encoding, high_encoding]:\n",
    "                for geno2 in [low_encoding, high_encoding]:\n",
    "                    # Create mask for this genotype combination\n",
    "                    mask = (genotype1_values == geno1) & (genotype2_values == geno2)\n",
    "\n",
    "                    if np.any(mask):\n",
    "                        # Get mean sensitivities for this genotype combination\n",
    "                        mean_sens1 = np.mean(locus1_sensitivities[mask])\n",
    "                        mean_sens2 = np.mean(locus2_sensitivities[mask])\n",
    "                        count = np.sum(mask)\n",
    "\n",
    "                        # Create a row for the summary DataFrame\n",
    "                        summary_rows.append({\n",
    "                            'phenotype': phenotype_name,\n",
    "                            'phenotype_idx': idx,\n",
    "                            'locus1': locus1,\n",
    "                            'locus2': locus2,\n",
    "                            'genotype1': binary_geno1,  # Store as 0/1 for consistency\n",
    "                            'genotype2': binary_geno2,  # Store as 0/1 for consistency\n",
    "                            'raw_genotype1': geno1,     # Store actual -1/1 values\n",
    "                            'raw_genotype2': geno2,     # Store actual -1/1 values\n",
    "                            'add_eff1': pair['add_eff1'],\n",
    "                            'add_eff2': pair['add_eff2'],\n",
    "                            'epi_eff': pair['epi_eff'],\n",
    "                            'abs_epi_eff': pair['abs_epi_eff'],\n",
    "                            'mean_sens1': mean_sens1,\n",
    "                            'mean_sens2': mean_sens2,\n",
    "                            'count': count,\n",
    "                            'selection_type': selection_type\n",
    "                        })\n",
    "\n",
    "    # Create the summary DataFrame\n",
    "    summary_df = pd.DataFrame(summary_rows)\n",
    "\n",
    "    # Add some useful calculated columns\n",
    "    if len(summary_df) > 0:\n",
    "        # Calculate covariance between locus1 and locus2 sensitivities for each pair\n",
    "        for idx in phenotype_idx:\n",
    "            for pair in top_pairs[idx]:\n",
    "                locus1 = pair['locus1']\n",
    "                locus2 = pair['locus2']\n",
    "                pair_key = (idx, locus1, locus2)\n",
    "\n",
    "                # Get sensitivities and genotypes\n",
    "                sens1 = results[pair_key]['locus1_sensitivities']\n",
    "                sens2 = results[pair_key]['locus2_sensitivities']\n",
    "\n",
    "                # Update all rows for this pair\n",
    "                mask = (summary_df['phenotype_idx'] == idx) & (summary_df['locus1'] == locus1) & (summary_df['locus2'] == locus2)\n",
    "\n",
    "\n",
    "        # Add combined genotype column for convenience\n",
    "        summary_df['genotype_combined'] = summary_df['genotype1'].astype(str) + summary_df['genotype2'].astype(str)\n",
    "\n",
    "    # Plot summary visualizations if requested\n",
    "    if plot_summary and results:\n",
    "        # Calculate max number of pairs across all phenotypes\n",
    "        max_pairs = max(len(pairs) for pairs in top_pairs.values())\n",
    "        num_cols = min(top_n, max_pairs)\n",
    "\n",
    "        # Skip plotting if no pairs were found\n",
    "        if num_cols == 0:\n",
    "            print(\"No pairs to plot.\")\n",
    "            return results, summary_df\n",
    "\n",
    "        # Create figure with subplots for each phenotype\n",
    "        fig, axes = plt.subplots(len(phenotype_idx), num_cols,\n",
    "                                figsize=(4*num_cols, 4*len(phenotype_idx)),\n",
    "                                squeeze=False)\n",
    "\n",
    "        # For each phenotype\n",
    "        for i, idx in enumerate(phenotype_idx):\n",
    "            # For each pair\n",
    "            for j, pair in enumerate(top_pairs[idx]):\n",
    "                locus1 = pair['locus1']\n",
    "                locus2 = pair['locus2']\n",
    "                pair_key = (idx, locus1, locus2)\n",
    "\n",
    "                # Skip if pair not in results\n",
    "                if pair_key not in results:\n",
    "                    continue\n",
    "\n",
    "                # Get data\n",
    "                pair_data = results[pair_key]\n",
    "                locus1_sens = pair_data['locus1_sensitivities']\n",
    "                locus2_sens = pair_data['locus2_sensitivities']\n",
    "                combined_geno = pair_data['combined_genotypes']\n",
    "\n",
    "                # Create scatter plot\n",
    "                ax = axes[i, j]\n",
    "\n",
    "                # Create scatter plot with different colors for each genotype combination\n",
    "                # We use low/high terminology for labels to be encoding-agnostic\n",
    "                genotype_labels = [f'{low_encoding}/{low_encoding}',\n",
    "                                  f'{low_encoding}/{high_encoding}',\n",
    "                                  f'{high_encoding}/{low_encoding}',\n",
    "                                  f'{high_encoding}/{high_encoding}']\n",
    "                colors = ['blue', 'green', 'orange', 'red']\n",
    "\n",
    "                for geno in range(4):\n",
    "                    mask = combined_geno == geno\n",
    "                    if np.any(mask):\n",
    "                        ax.scatter(\n",
    "                            locus1_sens[mask],\n",
    "                            locus2_sens[mask],\n",
    "                            c=colors[geno],\n",
    "                            label=genotype_labels[geno],\n",
    "                            alpha=0.7\n",
    "                        )\n",
    "\n",
    "                # Add labels and title\n",
    "                ax.set_xlabel(f'Locus {locus1} Sensitivity')\n",
    "                ax.set_ylabel(f'Locus {locus2} Sensitivity')\n",
    "                epi_mag = pair['abs_epi_eff']\n",
    "                title = f'Phenotype {idx+1}: Loci {locus1}/{locus2}\\nEpi Effect: {pair[\"epi_eff\"]:.3f}'\n",
    "                ax.set_title(title)\n",
    "\n",
    "                # Add legend\n",
    "                ax.legend(title=\"Genotypes\")\n",
    "\n",
    "                # Add grid\n",
    "                ax.grid(True, alpha=0.3)\n",
    "\n",
    "        # Add title indicating selection type\n",
    "        selection_title = f\"{selection_type.capitalize()} {top_n} Epistatic Pairs by Magnitude\"\n",
    "        fig.suptitle(selection_title, fontsize=16, y=1.02)\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "    return results, summary_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abf75b6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "epi_jacobian, epi_summ = extract_top_epistatic_pairs(model, test_loader_gp, true_eff,\n",
    "                               top_n=5, phenotype_idx=[1], device=device,\n",
    "                               max_samples=2000, plot_summary=True, selection_type = 'top')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b14f6ab5",
   "metadata": {},
   "source": [
    "Estimate epistatic effects for all possible pairs of loci by looking at values of specific genotype combinations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbfacad7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_all_epistatic_pairs(model, validation_loader, true_effects_df,\n",
    "                               phenotype_idx=None, device=device,\n",
    "                               max_samples=1000):\n",
    "    \"\"\"\n",
    "    Extract sensitivities for all possible unique pairs of loci for each phenotype.\n",
    "    For pairs with known epistatic effects in true_effects_df, include those effects.\n",
    "    For pairs without known epistatic effects, set the effect to 0.\n",
    "\n",
    "    Uses 1-based locus indexing throughout.\n",
    "\n",
    "    Args:\n",
    "        model: Trained neural network model\n",
    "        validation_loader: DataLoader for validation data\n",
    "        true_effects_df: DataFrame containing ground truth effects with columns:\n",
    "                         trait (1-based phenotype), locus (1-based), add_eff,\n",
    "                         epi_loc (1-based interacting locus), epi_eff\n",
    "        phenotype_idx: List of phenotype indices to analyze (None for all phenotypes)\n",
    "        device: Device to run calculations on\n",
    "        max_samples: Maximum number of samples to process\n",
    "\n",
    "    Returns:\n",
    "        DataFrame with one row per phenotype/genotype/locus pair\n",
    "    \"\"\"\n",
    "    # Ensure true_effects_df has required columns\n",
    "    required_cols = ['trait', 'locus', 'epi_loc', 'epi_eff']\n",
    "    if not all(col in true_effects_df.columns for col in required_cols):\n",
    "        raise ValueError(f\"true_effects_df must have these columns: {required_cols}\")\n",
    "\n",
    "    # Check for additive effects columns\n",
    "    if 'add_eff' in true_effects_df.columns:\n",
    "        add_eff_col = 'add_eff'\n",
    "\n",
    "    # Determine phenotype indices to analyze\n",
    "    if phenotype_idx is None:\n",
    "        # Get all unique phenotypes from true_effects_df\n",
    "        all_traits = true_effects_df['trait'].unique()\n",
    "        # Use 1-based indexing for phenotypes\n",
    "        phenotype_idx = list(all_traits)\n",
    "    elif isinstance(phenotype_idx, int):\n",
    "        phenotype_idx = [phenotype_idx]\n",
    "\n",
    "    print(f\"Analyzing phenotypes: {phenotype_idx}\")\n",
    "\n",
    "    # Create detached version of model for Jacobian calculation\n",
    "    detached_model = GP_net(\n",
    "        n_loci=n_loci,\n",
    "        hidden_layer_size=hidden_size,\n",
    "        n_pheno=n_phen,\n",
    "        detach_gate=True\n",
    "        )\n",
    "\n",
    "    detached_model.load_state_dict(model.state_dict())\n",
    "    detached_model = detached_model.to(device)\n",
    "\n",
    "    # Function to get importance for a specific phenotype\n",
    "    def get_phenotype_importance(x, pheno_idx):\n",
    "        # Adjust for 1-based phenotype indexing\n",
    "        model_pheno_idx = pheno_idx - 1  # Convert 1-based to 0-based for the model\n",
    "\n",
    "        def phenotype_fn(inp):\n",
    "            return detached_model(inp)[:, model_pheno_idx]\n",
    "\n",
    "        # Calculate Jacobian\n",
    "        jacobian = torch.autograd.functional.jacobian(\n",
    "            phenotype_fn,\n",
    "            x,\n",
    "            vectorize=True\n",
    "        ).squeeze()\n",
    "\n",
    "        # Return unmodified Jacobian value\n",
    "        return jacobian\n",
    "\n",
    "    # Get number of loci from the model\n",
    "    n_loci_model = n_loci  # This should be defined in your environment or passed to the function\n",
    "\n",
    "    # Create mapping of known epistatic effects\n",
    "    epistatic_effects = {}\n",
    "    additive_effects = {}\n",
    "\n",
    "    # First, collect all additive effects\n",
    "    for idx in phenotype_idx:\n",
    "        additive_effects[idx] = {}\n",
    "        pheno_effects = true_effects_df[true_effects_df['trait'] == idx]\n",
    "\n",
    "        if add_eff_col is not None:\n",
    "            # Get unique loci and their additive effects\n",
    "            for _, row in pheno_effects.drop_duplicates('locus').iterrows():\n",
    "                locus = int(row['locus'])\n",
    "                add_eff = row[add_eff_col]\n",
    "                additive_effects[idx][locus] = add_eff\n",
    "\n",
    "    # Now collect all epistatic effects\n",
    "    for idx in phenotype_idx:\n",
    "        epistatic_effects[idx] = {}\n",
    "        pheno_effects = true_effects_df[true_effects_df['trait'] == idx]\n",
    "\n",
    "        # Process each row to extract epistatic effects\n",
    "        for _, row in pheno_effects.iterrows():\n",
    "            locus1 = int(row['locus'])\n",
    "            locus2 = int(row['epi_loc'])\n",
    "\n",
    "            # Create a canonical representation of the pair (smaller locus first)\n",
    "            pair_key = tuple(sorted([locus1, locus2]))\n",
    "\n",
    "            # Store the epistatic effect\n",
    "            epistatic_effects[idx][pair_key] = row['epi_eff']\n",
    "\n",
    "    # Generate all possible locus pairs\n",
    "    all_pairs = {}\n",
    "    for idx in phenotype_idx:\n",
    "        pairs = []\n",
    "\n",
    "        # Generate all possible pairs of loci (using 1-based indexing)\n",
    "        for locus1 in range(1, n_loci_model + 1):  # Start from 1, not 0\n",
    "            for locus2 in range(locus1 + 1, n_loci_model + 1):  # Only use each pair once\n",
    "                # Create canonical pair key\n",
    "                pair_key = (locus1, locus2)\n",
    "\n",
    "                # Get epistatic effect if it exists, otherwise use 0\n",
    "                epi_eff = epistatic_effects[idx].get(pair_key, 0.0)\n",
    "                abs_epi_eff = abs(epi_eff)\n",
    "\n",
    "                # Get additive effects\n",
    "                add_eff1 = additive_effects[idx].get(locus1, 0.0)\n",
    "                add_eff2 = additive_effects[idx].get(locus2, 0.0)\n",
    "\n",
    "                # Store pair info\n",
    "                pairs.append({\n",
    "                    'locus1': locus1,\n",
    "                    'locus2': locus2,\n",
    "                    'epi_eff': epi_eff,\n",
    "                    'abs_epi_eff': abs_epi_eff,\n",
    "                    'add_eff1': add_eff1,\n",
    "                    'add_eff2': add_eff2,\n",
    "                    'has_epistasis': epi_eff != 0.0\n",
    "                })\n",
    "\n",
    "        all_pairs[idx] = pairs\n",
    "        epistatic_count = sum(1 for p in pairs if p['has_epistasis'])\n",
    "        print(f\"Generated {len(pairs)} total locus pairs for phenotype {idx}, {epistatic_count} with known epistatic effects\")\n",
    "\n",
    "    # Process validation samples to get sensitivities and genotypes\n",
    "    sample_indices = []\n",
    "    genotypes_dict = {}\n",
    "    sensitivities_dict = defaultdict(lambda: defaultdict(list))\n",
    "\n",
    "    # Process validation samples\n",
    "    sample_count = 0\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (phens, gens) in enumerate(validation_loader):\n",
    "            # Process each sample in the batch\n",
    "            for i in range(gens.shape[0]):\n",
    "                # Check if we've reached the maximum samples\n",
    "                if max_samples is not None and sample_count >= max_samples:\n",
    "                    break\n",
    "\n",
    "                # Store sample index\n",
    "                sample_indices.append(sample_count)\n",
    "\n",
    "                # Get single sample\n",
    "                single_gens = gens[i:i+1]\n",
    "\n",
    "                # Flatten and simplify (remove one-hot encoding)\n",
    "                flattened_gens = single_gens.reshape(single_gens.shape[0], -1)\n",
    "                simplified_gens = flattened_gens[:, 1::2].to(device)\n",
    "\n",
    "                # Store the raw genotype data\n",
    "                genotype = simplified_gens.cpu().numpy().squeeze()\n",
    "                genotypes_dict[sample_count] = genotype\n",
    "\n",
    "                # Calculate importance for each phenotype\n",
    "                for idx in phenotype_idx:\n",
    "                    importance = get_phenotype_importance(simplified_gens, idx)\n",
    "                    importance_np = importance.cpu().numpy()\n",
    "\n",
    "                    # Store sensitivity values for each locus in all pairs\n",
    "                    for pair in all_pairs[idx]:\n",
    "                        locus1 = pair['locus1']\n",
    "                        locus2 = pair['locus2']\n",
    "                        pair_key = (idx, locus1, locus2)\n",
    "\n",
    "                        # Convert from 1-based to 0-based for accessing the importance array\n",
    "                        model_locus1 = locus1 - 1\n",
    "                        model_locus2 = locus2 - 1\n",
    "\n",
    "                        # Store sensitivity values\n",
    "                        sensitivities_dict[pair_key]['locus1'].append(importance_np[model_locus1])\n",
    "                        sensitivities_dict[pair_key]['locus2'].append(importance_np[model_locus2])\n",
    "\n",
    "                sample_count += 1\n",
    "\n",
    "                # Print progress\n",
    "                if sample_count % 1000 == 0:\n",
    "                    print(f\"Processed {sample_count} samples\")\n",
    "\n",
    "            # Check again after batch\n",
    "            if max_samples is not None and sample_count >= max_samples:\n",
    "                break\n",
    "\n",
    "    print(f\"Completed analysis for {sample_count} samples\")\n",
    "\n",
    "    # Prepare final results\n",
    "    summary_rows = []  # For the summarized DataFrame\n",
    "\n",
    "    # Get phenotype names (trait names) if available\n",
    "    trait_names = {}\n",
    "    if 'trait_name' in true_effects_df.columns:\n",
    "        for _, row in true_effects_df.drop_duplicates('trait').iterrows():\n",
    "            trait_names[row['trait']] = row['trait_name']  # Keep 1-based indexing\n",
    "\n",
    "    for idx in phenotype_idx:\n",
    "        # Get phenotype name if available, otherwise use index\n",
    "        phenotype_name = trait_names.get(idx, f\"Phenotype_{idx}\")\n",
    "\n",
    "        for pair in all_pairs[idx]:\n",
    "            locus1 = pair['locus1']\n",
    "            locus2 = pair['locus2']\n",
    "            pair_key = (idx, locus1, locus2)\n",
    "\n",
    "            # Get sensitivity values\n",
    "            locus1_sensitivities = np.array(sensitivities_dict[pair_key]['locus1'])\n",
    "            locus2_sensitivities = np.array(sensitivities_dict[pair_key]['locus2'])\n",
    "\n",
    "            # Get genotypes for this pair\n",
    "            genotype1_values = []\n",
    "            genotype2_values = []\n",
    "            combined_genotypes = []\n",
    "\n",
    "            for s_idx in sample_indices:\n",
    "                genotype = genotypes_dict[s_idx]\n",
    "\n",
    "                # Convert from 1-based to 0-based for accessing genotype array\n",
    "                model_locus1 = locus1 - 1\n",
    "                model_locus2 = locus2 - 1\n",
    "\n",
    "                # Extract genotype values for the two loci\n",
    "                geno1 = genotype[model_locus1]\n",
    "                geno2 = genotype[model_locus2]\n",
    "\n",
    "                # Store individual genotypes\n",
    "                genotype1_values.append(geno1)\n",
    "                genotype2_values.append(geno2)\n",
    "\n",
    "                # Encode combined genotype for -1/1 encoding\n",
    "                # Map (-1,-1) → 0, (-1,1) → 1, (1,-1) → 2, (1,1) → 3\n",
    "                combined = ((geno1 + 1) // 2) * 2 + ((geno2 + 1) // 2)\n",
    "                combined_genotypes.append(combined)\n",
    "\n",
    "            # Convert to numpy arrays\n",
    "            genotype1_values = np.array(genotype1_values)\n",
    "            genotype2_values = np.array(genotype2_values)\n",
    "            combined_genotypes = np.array(combined_genotypes)\n",
    "\n",
    "            # Calculate mean sensitivity values for each genotype combination\n",
    "            for geno1 in [-1, 1]:\n",
    "                for geno2 in [-1, 1]:\n",
    "                    # Create mask for this genotype combination\n",
    "                    mask = (genotype1_values == geno1) & (genotype2_values == geno2)\n",
    "\n",
    "                    if np.any(mask):\n",
    "                        # Get mean sensitivities for this genotype combination\n",
    "                        mean_sens1 = np.mean(locus1_sensitivities[mask])\n",
    "                        mean_sens2 = np.mean(locus2_sensitivities[mask])\n",
    "                        count = np.sum(mask)\n",
    "\n",
    "                        # Create a row for the summary DataFrame\n",
    "                        summary_rows.append({\n",
    "                            'phenotype': phenotype_name,\n",
    "                            'phenotype_idx': idx,\n",
    "                            'locus1': locus1,\n",
    "                            'locus2': locus2,\n",
    "                            'genotype1': geno1,\n",
    "                            'genotype2': geno2,\n",
    "                            'add_eff1': pair['add_eff1'],\n",
    "                            'add_eff2': pair['add_eff2'],\n",
    "                            'epi_eff': pair['epi_eff'],\n",
    "                            'abs_epi_eff': pair['abs_epi_eff'],\n",
    "                            'mean_sens1': mean_sens1,\n",
    "                            'mean_sens2': mean_sens2,\n",
    "                            'count': count,\n",
    "                            'has_epistasis': pair['has_epistasis']\n",
    "                        })\n",
    "\n",
    "    # Create the summary DataFrame\n",
    "    summary_df = pd.DataFrame(summary_rows)\n",
    "\n",
    "    # Add some useful calculated columns\n",
    "    if len(summary_df) > 0:\n",
    "        # Calculate covariance between locus1 and locus2 sensitivities for each pair\n",
    "        for idx in phenotype_idx:\n",
    "            for pair in all_pairs[idx]:\n",
    "                locus1 = pair['locus1']\n",
    "                locus2 = pair['locus2']\n",
    "                pair_key = (idx, locus1, locus2)\n",
    "\n",
    "                if pair_key in sensitivities_dict:\n",
    "                    # Get sensitivities\n",
    "                    sens1 = np.array(sensitivities_dict[pair_key]['locus1'])\n",
    "                    sens2 = np.array(sensitivities_dict[pair_key]['locus2'])\n",
    "\n",
    "                    # Update all rows for this pair\n",
    "                    mask = (summary_df['phenotype_idx'] == idx) & (summary_df['locus1'] == locus1) & (summary_df['locus2'] == locus2)\n",
    "\n",
    "        # Add combined genotype column for convenience\n",
    "        summary_df['genotype_combined'] = summary_df['genotype1'].astype(str) + summary_df['genotype2'].astype(str)\n",
    "\n",
    "        # Calculate weighted means across genotypes\n",
    "        # Create unique pair identifiers\n",
    "        summary_df['pair_id'] = summary_df['phenotype_idx'].astype(str) + '_' + \\\n",
    "                               summary_df['locus1'].astype(str) + '_' + \\\n",
    "                               summary_df['locus2'].astype(str)\n",
    "\n",
    "        # Add weighted mean sensitivities\n",
    "        weighted_means = summary_df.groupby('pair_id').apply(\n",
    "            lambda x: pd.Series({\n",
    "                'weighted_mean_sens1': (x['mean_sens1'] * x['count']).sum() / x['count'].sum(),\n",
    "                'weighted_mean_sens2': (x['mean_sens2'] * x['count']).sum() / x['count'].sum(),\n",
    "                'total_samples': x['count'].sum()\n",
    "            })\n",
    "        )\n",
    "\n",
    "        # Merge weighted means back to summary_df\n",
    "        summary_df = summary_df.merge(weighted_means, on='pair_id')\n",
    "\n",
    "    return summary_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adc321cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def estimate_effects_from_sensitivities(summary_df, heritability=1.0):\n",
    "    \"\"\"\n",
    "    Estimate additive and epistatic effects from locus sensitivity means across genotype combinations.\n",
    "    Works with either 0/1 or -1/1 genotype encoding.\n",
    "    Applies appropriate sign corrections and heritability scaling.\n",
    "\n",
    "    Args:\n",
    "        summary_df: DataFrame output from extract_top_epistatic_pairs or extract_all_epistatic_pairs,\n",
    "                   containing mean sensitivity values for each locus/genotype combination\n",
    "        heritability: The broad-sense heritability value used for the phenotype (default=1.0)\n",
    "\n",
    "    Returns:\n",
    "        DataFrame with estimated additive and epistatic effects for each locus pair,\n",
    "        alongside true values for comparison\n",
    "    \"\"\"\n",
    "    # Create unique identifier for each locus pair\n",
    "    summary_df['pair_id'] = summary_df['phenotype_idx'].astype(str) + '_' + \\\n",
    "                           summary_df['locus1'].astype(str) + '_' + \\\n",
    "                           summary_df['locus2'].astype(str)\n",
    "\n",
    "    # Calculate heritability scaling factor\n",
    "    h_sqrt = np.sqrt(heritability)\n",
    "    print(f\"Using heritability scaling factor: {h_sqrt:.4f} (h² = {heritability:.2f})\")\n",
    "\n",
    "    # Determine genotype encoding by checking values in the dataframe\n",
    "    unique_genotypes = set(summary_df['genotype1'].unique()) | set(summary_df['genotype2'].unique())\n",
    "    if -1 in unique_genotypes:\n",
    "        # Using -1/1 encoding\n",
    "        encoding_type = \"-1/1\"\n",
    "        genotype_mapping = {\n",
    "            (-1, -1): \"00\",\n",
    "            (-1, 1): \"01\",\n",
    "            (1, -1): \"10\",\n",
    "            (1, 1): \"11\"\n",
    "        }\n",
    "    else:\n",
    "        # Using 0/1 encoding\n",
    "        encoding_type = \"0/1\"\n",
    "        genotype_mapping = {\n",
    "            (0, 0): \"00\",\n",
    "            (0, 1): \"01\",\n",
    "            (1, 0): \"10\",\n",
    "            (1, 1): \"11\"\n",
    "        }\n",
    "\n",
    "    print(f\"Detected genotype encoding: {encoding_type}\")\n",
    "\n",
    "    # Initialize results list\n",
    "    results = []\n",
    "\n",
    "    # Process each unique locus pair\n",
    "    for pair_id in summary_df['pair_id'].unique():\n",
    "        # Get data for this pair\n",
    "        pair_data = summary_df[summary_df['pair_id'] == pair_id].copy()\n",
    "\n",
    "        # Extract true effects (same for all rows of this pair)\n",
    "        true_add_eff1 = pair_data['add_eff1'].iloc[0]\n",
    "        true_add_eff2 = pair_data['add_eff2'].iloc[0]\n",
    "        true_epi_eff = pair_data['epi_eff'].iloc[0]\n",
    "        phenotype = pair_data['phenotype'].iloc[0]\n",
    "        phenotype_idx = pair_data['phenotype_idx'].iloc[0]\n",
    "        locus1 = pair_data['locus1'].iloc[0]\n",
    "        locus2 = pair_data['locus2'].iloc[0]\n",
    "\n",
    "        # Apply heritability scaling to true effects\n",
    "        scaled_true_add_eff1 = true_add_eff1 * h_sqrt if true_add_eff1 is not None else None\n",
    "        scaled_true_add_eff2 = true_add_eff2 * h_sqrt if true_add_eff2 is not None else None\n",
    "        scaled_true_epi_eff = true_epi_eff * h_sqrt if true_epi_eff is not None else None\n",
    "\n",
    "        # Create dictionary to store sensitivity means by genotype\n",
    "        sens1_by_genotype = {}\n",
    "        sens2_by_genotype = {}\n",
    "\n",
    "        # Fill dictionary with sensitivity means for each genotype combination\n",
    "        for _, row in pair_data.iterrows():\n",
    "            geno1 = row['genotype1']\n",
    "            geno2 = row['genotype2']\n",
    "            genotype_key = genotype_mapping.get((geno1, geno2), f\"{geno1}{geno2}\")\n",
    "            sens1_by_genotype[genotype_key] = row['mean_sens1']\n",
    "            sens2_by_genotype[genotype_key] = row['mean_sens2']\n",
    "\n",
    "        # Check if we have all four genotype combinations\n",
    "        required_genotypes = ['00', '01', '10', '11']\n",
    "        if not all(g in sens1_by_genotype and g in sens2_by_genotype for g in required_genotypes):\n",
    "            print(f\"Skipping pair {pair_id} - missing genotype combinations\")\n",
    "            continue\n",
    "\n",
    "        # 1. Estimate additive effects from mean sensitivity across all genotypes\n",
    "        # For locus 1: a₁ = mean(s₁)/2  (removed negative sign)\n",
    "        est_add_eff1 = np.mean([sens1_by_genotype[g] for g in required_genotypes]) / 2\n",
    "\n",
    "        # For locus 2: a₂ = mean(s₂)/2  (removed negative sign)\n",
    "        est_add_eff2 = np.mean([sens2_by_genotype[g] for g in required_genotypes]) / 2\n",
    "\n",
    "        # 2. Estimate epistatic effect with sign correction\n",
    "        # Method 1: From locus 1 sensitivities with sign correction\n",
    "        epi_est1 = -1 * (sens1_by_genotype['00'] - sens1_by_genotype['01'] +\n",
    "                         sens1_by_genotype['10'] - sens1_by_genotype['11']) / 4\n",
    "\n",
    "        # Method 2: From locus 2 sensitivities with sign correction\n",
    "        epi_est2 = -1 * (sens2_by_genotype['00'] - sens2_by_genotype['10'] +\n",
    "                         sens2_by_genotype['01'] - sens2_by_genotype['11']) / 4\n",
    "\n",
    "        # Average both estimates\n",
    "        est_epi_eff = (epi_est1 + epi_est2) / 2\n",
    "\n",
    "        # 3. Alternative estimate of epistasis from variance of sensitivities\n",
    "        # Variance should be proportional to 4e²\n",
    "        var_sens1 = np.var([sens1_by_genotype[g] for g in required_genotypes])\n",
    "        var_sens2 = np.var([sens2_by_genotype[g] for g in required_genotypes])\n",
    "\n",
    "        # e = √(Var[s])/2\n",
    "        epi_from_var1 = np.sqrt(var_sens1) / 2\n",
    "        epi_from_var2 = np.sqrt(var_sens2) / 2\n",
    "        est_epi_from_var = (epi_from_var1 + epi_from_var2) / 2\n",
    "\n",
    "        # Determine sign of epistatic effect from pattern with correction\n",
    "        # Note the sign flip here compared to original\n",
    "        epi_sign = -1 * np.sign(sens1_by_genotype['00'] - sens1_by_genotype['01'])\n",
    "        est_epi_from_var *= epi_sign\n",
    "\n",
    "\n",
    "        # 6. Store results\n",
    "        results.append({\n",
    "            'phenotype': phenotype,\n",
    "            'phenotype_idx': phenotype_idx,\n",
    "            'locus1': locus1,\n",
    "            'locus2': locus2,\n",
    "            'pair_id': pair_id,\n",
    "\n",
    "            # Original true effects\n",
    "            'true_add_eff1': true_add_eff1,\n",
    "            'true_add_eff2': true_add_eff2,\n",
    "            'true_epi_eff': true_epi_eff,\n",
    "\n",
    "            # Scaled true effects (adjusted for heritability)\n",
    "            'scaled_true_add_eff1': scaled_true_add_eff1,\n",
    "            'scaled_true_add_eff2': scaled_true_add_eff2,\n",
    "            'scaled_true_epi_eff': scaled_true_epi_eff,\n",
    "\n",
    "            # Estimated effects (already sign-corrected)\n",
    "            'est_add_eff1': est_add_eff1,\n",
    "            'est_add_eff2': est_add_eff2,\n",
    "            'est_epi_eff': est_epi_eff,\n",
    "            'est_epi_from_var': est_epi_from_var,\n",
    "\n",
    "            # Additional metrics\n",
    "            'var_sens1': var_sens1,\n",
    "            'var_sens2': var_sens2,\n",
    "\n",
    "            # Record heritability used\n",
    "            'heritability': heritability,\n",
    "\n",
    "            # Record encoding type\n",
    "            'encoding_type': encoding_type\n",
    "        })\n",
    "\n",
    "    # Create results DataFrame\n",
    "    results_df = pd.DataFrame(results)\n",
    "\n",
    "    return results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46e3c558",
   "metadata": {},
   "outputs": [],
   "source": [
    "#calculate genotype specific sensitivity means for all possible pairs of loci\n",
    "paired_all = extract_all_epistatic_pairs(model, test_loader_gp, true_eff,\n",
    "                               phenotype_idx=[1,2,3,4,5], device=device,\n",
    "                               max_samples=2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa1451f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#estimate quant-gen parameters\n",
    "qg_estimates = estimate_effects_from_sensitivities(paired_all, heritability=heritability)\n",
    "qg_estimates.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d343d8c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_phenotypes = qg_estimates['phenotype'].unique()\n",
    "\n",
    "# Create a figure with 5 subplots side by side\n",
    "fig, axes = plt.subplots(1, 5, figsize=(20, 4), sharey=True, sharex=False)\n",
    "fig.subplots_adjust(wspace=0.1)  # Adjust spacing between plots\n",
    "\n",
    "# Create a plot for each phenotype\n",
    "for i, phenotype in enumerate(unique_phenotypes):\n",
    "    # Filter data for this phenotype\n",
    "    phenotype_data = qg_estimates[qg_estimates['phenotype'] == phenotype]\n",
    "\n",
    "    # Create scatter plot on the appropriate subplot\n",
    "    axes[i].scatter(phenotype_data['true_epi_eff'], phenotype_data['est_epi_eff'],\n",
    "                   alpha=0.7)\n",
    "\n",
    "    if phenotype != 'Phenotype_1':\n",
    "        z = np.polyfit(phenotype_data['true_epi_eff'], phenotype_data['est_epi_eff'], 1)\n",
    "\n",
    "\n",
    "    # Add title and grid\n",
    "    axes[i].set_title(f'Phenotype {phenotype}')\n",
    "\n",
    "    if phenotype != 'Phenotype_1':\n",
    "        slope = z[0]  # Extract the slope from the polyfit result\n",
    "        axes[i].text(0.05, 0.95, f'Slope = {slope:.3f}',\n",
    "                    transform=axes[i].transAxes,\n",
    "                    verticalalignment='top',\n",
    "                    bbox=dict(boxstyle='round', facecolor='white', alpha=0.7))\n",
    "\n",
    "\n",
    "    # Only add y-label to the first plot\n",
    "    if i == 0:\n",
    "        axes[i].set_ylabel('Estimated Epistatic Effect')\n",
    "\n",
    "    # Add x-label to all plots\n",
    "    axes[i].set_xlabel('True Epistatic Effect')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fee9d71e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot histogram of epi estimates\n",
    "unique_phenotypes = qg_estimates['phenotype'].unique()\n",
    "\n",
    "# Create a figure with 5 subplots side by side\n",
    "fig, axes = plt.subplots(1, 5, figsize=(20, 4), sharex=False)\n",
    "fig.subplots_adjust(wspace=0.2)  # Adjust spacing between plots\n",
    "\n",
    "# Define bins for the histogram\n",
    "bins = 100  # Adjust this number as needed\n",
    "y_axis_limit = 25  # Set the y-axis limit here\n",
    "\n",
    "# Create a plot for each phenotype\n",
    "for i, phenotype in enumerate(unique_phenotypes):\n",
    "    # Filter data for this phenotype\n",
    "    phenotype_data = qg_estimates[qg_estimates['phenotype'] == phenotype]\n",
    "\n",
    "    # Define a threshold for \"true\" epistatic effects\n",
    "    threshold = 0.001\n",
    "\n",
    "    # Split data into true epistatic effects and false positives\n",
    "    true_epistatic = phenotype_data[abs(phenotype_data['true_epi_eff']) > threshold]\n",
    "    false_positive = phenotype_data[abs(phenotype_data['true_epi_eff']) <= threshold]\n",
    "\n",
    "    # Calculate histogram edges so we can use the same bins for both histograms\n",
    "    all_est_values = phenotype_data['est_epi_eff']\n",
    "    bin_edges = np.histogram_bin_edges(all_est_values, bins=bins)\n",
    "\n",
    "    # Create histograms for true and false epistatic effects\n",
    "    axes[i].hist([true_epistatic['est_epi_eff'], false_positive['est_epi_eff']],\n",
    "                bins=bin_edges, stacked=True, alpha=0.9,\n",
    "                color=['#4CAF50', '#F44336'],  # Green for true, red for false\n",
    "                label=['True Epistatic', 'False Positive'])\n",
    "\n",
    "    # Add title and grid\n",
    "    axes[i].set_title(f'Phenotype {phenotype}')\n",
    "    axes[i].grid(True, alpha=0.3)\n",
    "\n",
    "    # Set y-axis limit to see fine details in less populated bins\n",
    "    axes[i].set_ylim(0, y_axis_limit)\n",
    "\n",
    "    # Only add y-label to the first plot\n",
    "    if i == 0:\n",
    "        axes[i].set_ylabel('Count (capped at {})'.format(y_axis_limit))\n",
    "\n",
    "    # Add x-label to all plots\n",
    "    axes[i].set_xlabel('Estimated Epistatic Effect')\n",
    "\n",
    "    # Add legend only to the first plot to avoid redundancy\n",
    "    if i == 0:\n",
    "        axes[i].legend()\n",
    "\n",
    "    # Add a note about y-axis cap\n",
    "    if i == 0:\n",
    "        axes[i].text(0.05, 0.95, f'Y-axis capped at {y_axis_limit}',\n",
    "                    transform=axes[i].transAxes,\n",
    "                    fontsize=8, ha='left', va='top',\n",
    "                    bbox=dict(boxstyle='round', facecolor='white', alpha=0.7))\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dffe48a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_sample_similarity_matrix(importance_dfs, phenotype_idx=[0], max_samples=50, method='correlation'):\n",
    "    \"\"\"\n",
    "    Plot similarity matrix between samples based on their feature importance patterns.\n",
    "\n",
    "    Args:\n",
    "        importance_dfs: List of DataFrames with importance values, one per phenotype\n",
    "        phenotype_idx: List of indexes of phenotypes to analyze (default: [0] for first phenotype)\n",
    "        max_samples: Maximum number of samples to include (for visualization clarity)\n",
    "        method: 'correlation' or 'covariance' for similarity measure\n",
    "\n",
    "    Returns:\n",
    "        Tuple containing:\n",
    "        - List of similarity matrices, one per phenotype\n",
    "        - List of mean correlation values, one per phenotype (only if method='correlation')\n",
    "    \"\"\"\n",
    "    # Convert single index to list for consistency\n",
    "    if isinstance(phenotype_idx, int):\n",
    "        phenotype_idx = [phenotype_idx]\n",
    "\n",
    "    # Create figure for similarity matrices\n",
    "    fig_sim, axes_sim = plt.subplots(1, len(phenotype_idx), figsize=(len(phenotype_idx) * 10, 8))\n",
    "    if len(phenotype_idx) == 1:\n",
    "        axes_sim = [axes_sim]  # Convert to list for consistent indexing\n",
    "\n",
    "    # Create figure for correlation histograms if method is correlation\n",
    "    if method == 'correlation':\n",
    "        fig_hist, axes_hist = plt.subplots(1, len(phenotype_idx), figsize=(len(phenotype_idx) * 10, 6))\n",
    "        if len(phenotype_idx) == 1:\n",
    "            axes_hist = [axes_hist]  # Convert to list for consistent indexing\n",
    "\n",
    "    # Store similarity matrices and mean correlations\n",
    "    similarity_matrices = []\n",
    "    mean_correlations = []\n",
    "\n",
    "    for i, idx in enumerate(phenotype_idx):\n",
    "        # Get the dataframe for this phenotype\n",
    "        if isinstance(importance_dfs, list):\n",
    "            importance_df = importance_dfs[i]\n",
    "        else:\n",
    "            importance_df = importance_dfs[idx]\n",
    "\n",
    "        # Subset samples if needed\n",
    "        if importance_df.shape[0] > max_samples:\n",
    "            # Take a random subset of samples\n",
    "            sample_indices = np.random.choice(importance_df.shape[0], max_samples, replace=False)\n",
    "            subset_df = importance_df.iloc[sample_indices]\n",
    "        else:\n",
    "            subset_df = importance_df\n",
    "\n",
    "        # Transpose to get samples as columns\n",
    "        sample_df = subset_df.T\n",
    "\n",
    "        # Calculate similarity matrix\n",
    "        if method == 'correlation':\n",
    "            similarity_matrix = sample_df.corr()\n",
    "            title = f'Phenotype {idx+1}: Correlation of Feature Importance Between Samples'\n",
    "            label = 'Correlation'\n",
    "            vmin, vmax = -1, 1\n",
    "\n",
    "            # Calculate mean correlation (from upper triangle, excluding diagonal)\n",
    "            triu_indices = np.triu_indices_from(similarity_matrix, k=1)\n",
    "            corr_values = similarity_matrix.values[triu_indices]\n",
    "            mean_correlation = np.mean(corr_values)\n",
    "            mean_correlations.append(mean_correlation)\n",
    "\n",
    "        else:  # covariance\n",
    "            similarity_matrix = sample_df.cov()\n",
    "            title = f'Phenotype {idx+1}: Covariance of Feature Importance Between Samples'\n",
    "            label = 'Covariance'\n",
    "            # Set vmin/vmax for covariance to have white at zero\n",
    "            max_abs_val = np.max(np.abs(similarity_matrix.values))\n",
    "            vmin, vmax = -max_abs_val, max_abs_val\n",
    "\n",
    "            # Calculate mean correlation (from upper triangle, excluding diagonal)\n",
    "            triu_indices = np.triu_indices_from(similarity_matrix, k=1)\n",
    "            corr_values = similarity_matrix.values[triu_indices]\n",
    "            mean_correlation = np.mean(corr_values)\n",
    "            mean_correlations.append(mean_correlation)\n",
    "\n",
    "        # Store the similarity matrix\n",
    "        similarity_matrices.append(similarity_matrix)\n",
    "\n",
    "        # Plot heatmap\n",
    "        im = axes_sim[i].imshow(similarity_matrix, cmap='RdBu_r', vmin=vmin, vmax=vmax)\n",
    "        fig_sim.colorbar(im, ax=axes_sim[i], label=label)\n",
    "        axes_sim[i].set_title(title)\n",
    "        axes_sim[i].set_xlabel('Sample Index')\n",
    "        axes_sim[i].set_ylabel('Sample Index')\n",
    "\n",
    "        # Calculate summary statistics for correlation method\n",
    "        if method == 'correlation':\n",
    "            # Get upper triangle values (excluding diagonal)\n",
    "            triu_indices = np.triu_indices_from(similarity_matrix, k=1)\n",
    "            corr_values = similarity_matrix.values[triu_indices]\n",
    "\n",
    "            # Plot histogram of correlations\n",
    "            axes_hist[i].hist(corr_values, bins=30, alpha=0.7, color='steelblue')\n",
    "            axes_hist[i].axvline(0, color='red', linestyle='--', label=\"no corr\")\n",
    "            # Add mean correlation line\n",
    "            axes_hist[i].axvline(mean_correlation, color='green', linestyle='-',\n",
    "                                label=f\"mean corr: {mean_correlation:.4f}\")\n",
    "            axes_hist[i].set_xlabel('Correlation Value')\n",
    "            axes_hist[i].set_ylabel('Frequency')\n",
    "            axes_hist[i].set_title(f'Phenotype {idx+1}: Distribution of Sample-to-Sample Correlations')\n",
    "            axes_hist[i].legend()\n",
    "\n",
    "    # Adjust layout for all plots\n",
    "    fig_sim.tight_layout()\n",
    "    if method == 'correlation':\n",
    "        fig_hist.tight_layout()\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "    if method == 'correlation':\n",
    "        return similarity_matrices, mean_correlations\n",
    "    else:\n",
    "        return similarity_matrices, mean_correlations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "229ff526",
   "metadata": {},
   "outputs": [],
   "source": [
    "similarity_matrices, mean_correlations = plot_sample_similarity_matrix(\n",
    "     importance_results,\n",
    "     phenotype_idx=[0, 1, 2, 3, 4],\n",
    "     max_samples=50,  # Adjust as needed\n",
    "     method='correlation'  # 'covariance' or 'correlation'\n",
    " )\n",
    "_, mean_covariances = plot_sample_similarity_matrix(\n",
    "     importance_results,\n",
    "     phenotype_idx=[0, 1, 2, 3, 4],\n",
    "     max_samples=50,  # Adjust as needed\n",
    "     method='covariance'  # 'covariance' or 'correlation'\n",
    " )"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "ipynb,py:percent"
  },
  "kernelspec": {
   "display_name": "notebook-pub-template",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
